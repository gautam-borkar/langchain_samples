{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d25794",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-ollama langgraph geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cacecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "def get_llm():\n",
    "  return ChatOllama(model='granite3.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "  \"\"\"The state of the agent.\"\"\"\n",
    "  messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "  number_of_steps: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbcdd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from geopy.geocoders import Nominatim\n",
    "from pydantic import BaseModel, Field\n",
    "import requests\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"weather-app\")\n",
    "\n",
    "class SearchInput(BaseModel):\n",
    "  location:str = Field(description=\"The city and state, e.g., San Francisco\")\n",
    "  date:str = Field(description=\"The forecasting date for when to get the weather format (yyyy-mm-dd)\")\n",
    "\n",
    "@tool(\"get_weather_forecast\", args_schema=SearchInput, return_direct=True)\n",
    "def get_weather_forecast(location: str, date: str):\n",
    "  \"\"\"\n",
    "  Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). \n",
    "  Returns a list dictionary with the time and temperature for each hour.\n",
    "  \"\"\"\n",
    "  location = geolocator.geocode(location)\n",
    "  if location:\n",
    "    try:\n",
    "      response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={location.latitude}&longitude={location.longitude}&hourly=temperature_2m&start_date={date}&end_date={date}\")\n",
    "      data = response.json()\n",
    "      return {time: temp for time, temp in zip(data[\"hourly\"][\"time\"], data[\"hourly\"][\"temperature_2m\"])}\n",
    "    except Exception as e:\n",
    "      return {\"error\": str(e)}\n",
    "  else:\n",
    "    return {\"error\": \"Location not found\"}\n",
    "  \n",
    "tools = [get_weather_forecast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_llm()\n",
    "\n",
    "# Bind tools to the model\n",
    "model = model.bind_tools([get_weather_forecast])\n",
    "\n",
    "# Test the model with tools\n",
    "model.invoke(\"What is the weather in Berlin on 12th of March 2025?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80870cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.messages import ToolMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "\n",
    "# Define our tool node\n",
    "def call_tool(state: AgentState):\n",
    "    outputs = []\n",
    "\n",
    "    # Iterate over the tool calls in the last message\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "      # Get the tool by name\n",
    "      tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "      outputs.append(\n",
    "          ToolMessage(\n",
    "              content=tool_result,\n",
    "              name=tool_call[\"name\"],\n",
    "              tool_call_id=tool_call[\"id\"],\n",
    "          )\n",
    "      )\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "def call_model(state: AgentState, config: RunnableConfig):\n",
    "    # Invoke the model with the system prompt and the messages\n",
    "    response = model.invoke(state[\"messages\"], config)\n",
    "\n",
    "    # We return a list, because this will get added to the existing messages state using the add_messages reducer\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the conditional edge that determines whether to continue or not\n",
    "def should_continue(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    # If the last message is not a tool call, then we finish\n",
    "    if not messages[-1].tool_calls:\n",
    "        return \"end\"\n",
    "    # default to continue\n",
    "    return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define a new graph with our state\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# 1. Add our nodes\n",
    "workflow.add_node(\"llm\", call_model)\n",
    "workflow.add_node(\"tools\", call_tool)\n",
    "# 2. Set the entrypoint as `agent`, this is the first node called\n",
    "workflow.set_entry_point(\"llm\")\n",
    "# 3. Add a conditional edge after the `llm` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    # Edge is used after the `llm` node is called.\n",
    "    \"llm\",\n",
    "    # The function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Mapping for where to go next, keys are strings from the function return, and the values are other nodes.\n",
    "    # END is a special node marking that the graph is finish.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"tools\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "# 4. Add a normal edge after `tools` is called, `llm` node is called next.\n",
    "workflow.add_edge(\"tools\", \"llm\")\n",
    "\n",
    "# Now we can compile and visualize our graph\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6607516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eece2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our initial message dictionary\n",
    "inputs = {\"messages\": [(\"user\", \"How is the weather in Berlin on 22nd of April 2025?\")]}\n",
    "\n",
    "# call our graph with streaming to see the steps\n",
    "\n",
    "for state in graph.stream(inputs, stream_mode=\"values\"):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    last_message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffe38f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"messages\"].append((\"user\", \"Would it be warmer in Munich?\"))\n",
    "\n",
    "\n",
    "for state in graph.stream(state, stream_mode=\"values\"):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    last_message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
